{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c551ef2c-a5b5-4d4a-94f9-56fc2694ed91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T15:12:48.511229Z",
     "iopub.status.busy": "2024-08-16T15:12:48.510721Z",
     "iopub.status.idle": "2024-08-16T15:12:53.751502Z",
     "shell.execute_reply": "2024-08-16T15:12:53.750924Z",
     "shell.execute_reply.started": "2024-08-16T15:12:48.511211Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from bolero.tl.generic.module_lora import LoRAConv, convert_to_lora_model\n",
    "from enformer_pytorch.modeling_enformer import from_pretrained\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34943dba-b89c-492a-bcc5-3a8acb310b6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T15:12:53.752915Z",
     "iopub.status.busy": "2024-08-16T15:12:53.752412Z",
     "iopub.status.idle": "2024-08-16T15:12:53.757807Z",
     "shell.execute_reply": "2024-08-16T15:12:53.757375Z",
     "shell.execute_reply.started": "2024-08-16T15:12:53.752896Z"
    }
   },
   "outputs": [],
   "source": [
    "def summary_on_cuda(model, input_size=None, row_settings=[\"var_names\"], depth=4):\n",
    "    model.to(\"cuda\")\n",
    "    out = summary(model, input_size, row_settings=[\"var_names\"], depth=depth)\n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return out\n",
    "\n",
    "\n",
    "def print_lora_conv(model):\n",
    "    print(\"in_channels\", model.conv.in_channels)\n",
    "    print(\"out_channels\", model.conv.out_channels)\n",
    "    print(\"groups\", model.conv.groups)\n",
    "    print(\"kernel_size\", model.conv.kernel_size)\n",
    "    print()\n",
    "    print(\"LoRA Module\", type(model))\n",
    "    count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "        if param.requires_grad:\n",
    "            count += param.shape.numel()\n",
    "    print(\"LoRA trainable parameters: \", count)\n",
    "\n",
    "\n",
    "def print_lora_linear(model):\n",
    "    print(\"in_features\", model.in_features)\n",
    "    print(\"out_features\", model.out_features)\n",
    "    print()\n",
    "    print(\"LoRA Module\", type(model))\n",
    "    count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "        if param.requires_grad:\n",
    "            count += param.shape.numel()\n",
    "    print(\"LoRA trainable parameters: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fb7a8-686d-4aef-a387-ab9bb573f6d5",
   "metadata": {},
   "source": [
    "## Load Pretrained Enformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac29f0a-ee39-4cef-b2d3-9b70614e7e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:40.143840Z",
     "iopub.status.busy": "2024-08-05T03:06:40.143696Z",
     "iopub.status.idle": "2024-08-05T03:06:40.751197Z",
     "shell.execute_reply": "2024-08-05T03:06:40.750646Z",
     "shell.execute_reply.started": "2024-08-05T03:06:40.143827Z"
    }
   },
   "outputs": [],
   "source": [
    "enformer = from_pretrained(\"EleutherAI/enformer-official-rough\")\n",
    "\n",
    "for param in enformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd32996-dc45-442e-bf59-1aaba6ade58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:41.483700Z",
     "iopub.status.busy": "2024-08-05T03:06:41.483541Z",
     "iopub.status.idle": "2024-08-05T03:06:41.485995Z",
     "shell.execute_reply": "2024-08-05T03:06:41.485601Z",
     "shell.execute_reply.started": "2024-08-05T03:06:41.483685Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_rank = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659b991f-d2fb-48ce-8497-522b8f0af855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:41.987979Z",
     "iopub.status.busy": "2024-08-05T03:06:41.987801Z",
     "iopub.status.idle": "2024-08-05T03:06:44.640075Z",
     "shell.execute_reply": "2024-08-05T03:06:44.639450Z",
     "shell.execute_reply.started": "2024-08-05T03:06:41.987965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to a LoRA-enabled network\n",
    "lora_enformer = convert_to_lora_model(\n",
    "    enformer,\n",
    "    convert_conv=True,\n",
    "    convert_linear=True,\n",
    "    rank=lora_rank,\n",
    "    alpha=1.0,\n",
    "    inplace=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9426d556-456d-4cb0-b116-e4c71d73ffae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:45.307674Z",
     "iopub.status.busy": "2024-08-05T03:06:45.307226Z",
     "iopub.status.idle": "2024-08-05T03:06:45.312138Z",
     "shell.execute_reply": "2024-08-05T03:06:45.311633Z",
     "shell.execute_reply.started": "2024-08-05T03:06:45.307649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1848256\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for param in lora_enformer.parameters():\n",
    "    if param.requires_grad:\n",
    "        count += param.numel()\n",
    "print(\"Total trainable parameters:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5bb9e4-fba7-49e9-abf0-226cacc80a35",
   "metadata": {},
   "source": [
    "## Enformer stem\n",
    "\n",
    "- Input:\n",
    "  - shape: `(bs, channel, seq_len) or (16, 4, 196_608)`\n",
    "  - Seq length: 196_608 = 3 * 2** (9 + 1 + 6)\n",
    "- Output:\n",
    "  - shape: `(bs, C/2, seq_len/2) or (16, 768, 98_304)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79975e33-b24d-4433-a69c-d7f5a985960d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:46.683265Z",
     "iopub.status.busy": "2024-08-05T03:06:46.682834Z",
     "iopub.status.idle": "2024-08-05T03:06:47.451110Z",
     "shell.execute_reply": "2024-08-05T03:06:47.450439Z",
     "shell.execute_reply.started": "2024-08-05T03:06:46.683241Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enformer model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 768, 98304]          --\n",
       "├─Conv1d (0)                             [16, 768, 196608]         (46,848)\n",
       "├─Residual (1)                           [16, 768, 196608]         --\n",
       "│    └─Sequential (fn)                   [16, 768, 196608]         --\n",
       "│    │    └─BatchNorm1d (0)              [16, 768, 196608]         (1,536)\n",
       "│    │    └─GELU (1)                     [16, 768, 196608]         --\n",
       "│    │    └─Conv1d (2)                   [16, 768, 196608]         (590,592)\n",
       "├─AttentionPool (2)                      [16, 768, 98304]          --\n",
       "│    └─Rearrange (pool_fn)               [16, 768, 98304, 2]       --\n",
       "│    └─Conv2d (to_attn_logits)           [16, 768, 98304, 2]       (589,824)\n",
       "==========================================================================================\n",
       "Total params: 1,228,800\n",
       "Trainable params: 0\n",
       "Non-trainable params: 1,228,800\n",
       "Total mult-adds (T): 3.86\n",
       "==========================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 77309.41\n",
       "Params size (MB): 4.92\n",
       "Estimated Total Size (MB): 77364.66\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEnformer model\")\n",
    "summary_on_cuda(\n",
    "    enformer.stem, input_size=(16, 4, 196_608), row_settings=[\"var_names\"], depth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2d2fa6-4247-47c2-9a99-21bb45fc3b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:48.339709Z",
     "iopub.status.busy": "2024-08-05T03:06:48.339254Z",
     "iopub.status.idle": "2024-08-05T03:06:48.604888Z",
     "shell.execute_reply": "2024-08-05T03:06:48.604422Z",
     "shell.execute_reply.started": "2024-08-05T03:06:48.339683Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA-enabled Network:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 768, 98304]          --\n",
       "├─LoRAConv (0)                           [16, 768, 196608]         49,680\n",
       "│    └─Conv1d (conv)                     [16, 768, 196608]         (46,848)\n",
       "├─Residual (1)                           [16, 768, 196608]         --\n",
       "│    └─Sequential (fn)                   [16, 768, 196608]         --\n",
       "│    │    └─BatchNorm1d (0)              [16, 768, 196608]         (1,536)\n",
       "│    │    └─GELU (1)                     [16, 768, 196608]         --\n",
       "│    │    └─LoRAConv (2)                 [16, 768, 196608]         6,144\n",
       "│    │    │    └─Conv1d (conv)           [16, 768, 196608]         (590,592)\n",
       "├─AttentionPool (2)                      [16, 768, 98304]          --\n",
       "│    └─Rearrange (pool_fn)               [16, 768, 98304, 2]       --\n",
       "│    └─LoRAConv (to_attn_logits)         [16, 768, 98304, 2]       6,144\n",
       "│    │    └─Conv2d (conv)                [16, 768, 98304, 2]       (589,824)\n",
       "==========================================================================================\n",
       "Total params: 1,290,768\n",
       "Trainable params: 61,968\n",
       "Non-trainable params: 1,228,800\n",
       "Total mult-adds (T): 3.86\n",
       "==========================================================================================\n",
       "Input size (MB): 50.33\n",
       "Forward/backward pass size (MB): 77309.41\n",
       "Params size (MB): 4.92\n",
       "Estimated Total Size (MB): 77364.66\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoRA-enabled Network:\")\n",
    "summary_on_cuda(\n",
    "    lora_enformer.stem, input_size=(16, 4, 196_608), row_settings=[\"var_names\"], depth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31519493-1c49-4af4-a4b9-4031327e27ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LoRA modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aad3ba-43fb-409a-a6ef-f2f21b5c6341",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Stem DNA Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1a1152-454c-4176-9229-b18ee09fda92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:50.095538Z",
     "iopub.status.busy": "2024-08-05T03:06:50.095353Z",
     "iopub.status.idle": "2024-08-05T03:06:50.098293Z",
     "shell.execute_reply": "2024-08-05T03:06:50.097748Z",
     "shell.execute_reply.started": "2024-08-05T03:06:50.095524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 4\n",
      "out_channels 768\n",
      "groups 1\n",
      "kernel_size (15,)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([60, 60]) True\n",
      "lora_B torch.Size([768, 60]) True\n",
      "conv.weight torch.Size([768, 4, 15]) False\n",
      "conv.bias torch.Size([768]) False\n",
      "LoRA trainable parameters:  49680\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.stem[0]\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9b839-2827-4cdc-8928-ce8a3c46e004",
   "metadata": {},
   "source": [
    "#### Stem Linear Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58299269-9879-40e6-8020-5c465eb8c78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:50.975136Z",
     "iopub.status.busy": "2024-08-05T03:06:50.974777Z",
     "iopub.status.idle": "2024-08-05T03:06:50.977918Z",
     "shell.execute_reply": "2024-08-05T03:06:50.977470Z",
     "shell.execute_reply.started": "2024-08-05T03:06:50.975113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 768\n",
      "out_channels 768\n",
      "groups 1\n",
      "kernel_size (1,)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([4, 768]) True\n",
      "lora_B torch.Size([768, 4]) True\n",
      "conv.weight torch.Size([768, 768, 1]) False\n",
      "conv.bias torch.Size([768]) False\n",
      "LoRA trainable parameters:  6144\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.stem[1].fn[2]\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f8370-f198-409d-8a4d-d39629a1d874",
   "metadata": {},
   "source": [
    "#### Stem AttentionPool Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c313cce-d9e0-42a9-9c81-5eb091a82900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:51.855218Z",
     "iopub.status.busy": "2024-08-05T03:06:51.854861Z",
     "iopub.status.idle": "2024-08-05T03:06:51.857983Z",
     "shell.execute_reply": "2024-08-05T03:06:51.857530Z",
     "shell.execute_reply.started": "2024-08-05T03:06:51.855196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 768\n",
      "out_channels 768\n",
      "groups 1\n",
      "kernel_size (1, 1)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([4, 768]) True\n",
      "lora_B torch.Size([768, 4]) True\n",
      "conv.weight torch.Size([768, 768, 1, 1]) False\n",
      "LoRA trainable parameters:  6144\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.stem[2].to_attn_logits\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c8fca-2274-4c39-ab90-00f96001bfeb",
   "metadata": {},
   "source": [
    "## Enformer Conv Towers\n",
    "\n",
    "- Input:\n",
    "  - shape: `(bs, C/2, seq_len/2) or (16, 768, 98_304)`\n",
    "- Output:\n",
    "  - Output seq length 1536 = 196_608 / 2 ** 6\n",
    "  - shape: `(bs, C, seq_len/2**7) or (16, 1536, 1536)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925e4a42-3462-40db-a97b-1887291aaac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:06:52.503183Z",
     "iopub.status.busy": "2024-08-05T03:06:52.503009Z",
     "iopub.status.idle": "2024-08-05T03:07:00.450893Z",
     "shell.execute_reply": "2024-08-05T03:07:00.450276Z",
     "shell.execute_reply.started": "2024-08-05T03:06:52.503170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enformer model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 1536]          --\n",
       "├─Sequential (0)                         [16, 768, 49152]          --\n",
       "│    └─Sequential (0)                    [16, 768, 98304]          --\n",
       "│    │    └─BatchNorm1d (0)              [16, 768, 98304]          (1,536)\n",
       "│    │    └─GELU (1)                     [16, 768, 98304]          --\n",
       "│    │    └─Conv1d (2)                   [16, 768, 98304]          (2,949,888)\n",
       "│    └─Residual (1)                      [16, 768, 98304]          --\n",
       "│    │    └─Sequential (fn)              [16, 768, 98304]          --\n",
       "│    │    │    └─BatchNorm1d (0)         [16, 768, 98304]          (1,536)\n",
       "│    │    │    └─GELU (1)                [16, 768, 98304]          --\n",
       "│    │    │    └─Conv1d (2)              [16, 768, 98304]          (590,592)\n",
       "│    └─AttentionPool (2)                 [16, 768, 49152]          --\n",
       "│    │    └─Rearrange (pool_fn)          [16, 768, 49152, 2]       --\n",
       "│    │    └─Conv2d (to_attn_logits)      [16, 768, 49152, 2]       (589,824)\n",
       "├─Sequential (1)                         [16, 896, 24576]          --\n",
       "│    └─Sequential (0)                    [16, 896, 49152]          --\n",
       "│    │    └─BatchNorm1d (0)              [16, 768, 49152]          (1,536)\n",
       "│    │    └─GELU (1)                     [16, 768, 49152]          --\n",
       "│    │    └─Conv1d (2)                   [16, 896, 49152]          (3,441,536)\n",
       "│    └─Residual (1)                      [16, 896, 49152]          --\n",
       "│    │    └─Sequential (fn)              [16, 896, 49152]          --\n",
       "│    │    │    └─BatchNorm1d (0)         [16, 896, 49152]          (1,792)\n",
       "│    │    │    └─GELU (1)                [16, 896, 49152]          --\n",
       "│    │    │    └─Conv1d (2)              [16, 896, 49152]          (803,712)\n",
       "│    └─AttentionPool (2)                 [16, 896, 24576]          --\n",
       "│    │    └─Rearrange (pool_fn)          [16, 896, 24576, 2]       --\n",
       "│    │    └─Conv2d (to_attn_logits)      [16, 896, 24576, 2]       (802,816)\n",
       "├─Sequential (2)                         [16, 1024, 12288]         --\n",
       "│    └─Sequential (0)                    [16, 1024, 24576]         --\n",
       "│    │    └─BatchNorm1d (0)              [16, 896, 24576]          (1,792)\n",
       "│    │    └─GELU (1)                     [16, 896, 24576]          --\n",
       "│    │    └─Conv1d (2)                   [16, 1024, 24576]         (4,588,544)\n",
       "│    └─Residual (1)                      [16, 1024, 24576]         --\n",
       "│    │    └─Sequential (fn)              [16, 1024, 24576]         --\n",
       "│    │    │    └─BatchNorm1d (0)         [16, 1024, 24576]         (2,048)\n",
       "│    │    │    └─GELU (1)                [16, 1024, 24576]         --\n",
       "│    │    │    └─Conv1d (2)              [16, 1024, 24576]         (1,049,600)\n",
       "│    └─AttentionPool (2)                 [16, 1024, 12288]         --\n",
       "│    │    └─Rearrange (pool_fn)          [16, 1024, 12288, 2]      --\n",
       "│    │    └─Conv2d (to_attn_logits)      [16, 1024, 12288, 2]      (1,048,576)\n",
       "├─Sequential (3)                         [16, 1152, 6144]          --\n",
       "│    └─Sequential (0)                    [16, 1152, 12288]         --\n",
       "│    │    └─BatchNorm1d (0)              [16, 1024, 12288]         (2,048)\n",
       "│    │    └─GELU (1)                     [16, 1024, 12288]         --\n",
       "│    │    └─Conv1d (2)                   [16, 1152, 12288]         (5,899,392)\n",
       "│    └─Residual (1)                      [16, 1152, 12288]         --\n",
       "│    │    └─Sequential (fn)              [16, 1152, 12288]         --\n",
       "│    │    │    └─BatchNorm1d (0)         [16, 1152, 12288]         (2,304)\n",
       "│    │    │    └─GELU (1)                [16, 1152, 12288]         --\n",
       "│    │    │    └─Conv1d (2)              [16, 1152, 12288]         (1,328,256)\n",
       "│    └─AttentionPool (2)                 [16, 1152, 6144]          --\n",
       "│    │    └─Rearrange (pool_fn)          [16, 1152, 6144, 2]       --\n",
       "│    │    └─Conv2d (to_attn_logits)      [16, 1152, 6144, 2]       (1,327,104)\n",
       "├─Sequential (4)                         [16, 1280, 3072]          --\n",
       "│    └─Sequential (0)                    [16, 1280, 6144]          --\n",
       "│    │    └─BatchNorm1d (0)              [16, 1152, 6144]          (2,304)\n",
       "│    │    └─GELU (1)                     [16, 1152, 6144]          --\n",
       "│    │    └─Conv1d (2)                   [16, 1280, 6144]          (7,374,080)\n",
       "│    └─Residual (1)                      [16, 1280, 6144]          --\n",
       "│    │    └─Sequential (fn)              [16, 1280, 6144]          --\n",
       "│    │    │    └─BatchNorm1d (0)         [16, 1280, 6144]          (2,560)\n",
       "│    │    │    └─GELU (1)                [16, 1280, 6144]          --\n",
       "│    │    │    └─Conv1d (2)              [16, 1280, 6144]          (1,639,680)\n",
       "│    └─AttentionPool (2)                 [16, 1280, 3072]          --\n",
       "│    │    └─Rearrange (pool_fn)          [16, 1280, 3072, 2]       --\n",
       "│    │    └─Conv2d (to_attn_logits)      [16, 1280, 3072, 2]       (1,638,400)\n",
       "├─Sequential (5)                         [16, 1536, 1536]          --\n",
       "│    └─Sequential (0)                    [16, 1536, 3072]          --\n",
       "│    │    └─BatchNorm1d (0)              [16, 1280, 3072]          (2,560)\n",
       "│    │    └─GELU (1)                     [16, 1280, 3072]          --\n",
       "│    │    └─Conv1d (2)                   [16, 1536, 3072]          (9,831,936)\n",
       "│    └─Residual (1)                      [16, 1536, 3072]          --\n",
       "│    │    └─Sequential (fn)              [16, 1536, 3072]          --\n",
       "│    │    │    └─BatchNorm1d (0)         [16, 1536, 3072]          (3,072)\n",
       "│    │    │    └─GELU (1)                [16, 1536, 3072]          --\n",
       "│    │    │    └─Conv1d (2)              [16, 1536, 3072]          (2,360,832)\n",
       "│    └─AttentionPool (2)                 [16, 1536, 1536]          --\n",
       "│    │    └─Rearrange (pool_fn)          [16, 1536, 1536, 2]       --\n",
       "│    │    └─Conv2d (to_attn_logits)      [16, 1536, 1536, 2]       (2,359,296)\n",
       "==========================================================================================\n",
       "Total params: 49,649,152\n",
       "Trainable params: 0\n",
       "Non-trainable params: 49,649,152\n",
       "Total mult-adds (T): 16.54\n",
       "==========================================================================================\n",
       "Input size (MB): 4831.84\n",
       "Forward/backward pass size (MB): 108112.38\n",
       "Params size (MB): 198.60\n",
       "Estimated Total Size (MB): 113142.81\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEnformer model\")\n",
    "summary_on_cuda(\n",
    "    enformer.conv_tower,\n",
    "    input_size=(16, 768, 98_304),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b6ceee-3857-4ade-9339-806ad2dbdcd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:00.452186Z",
     "iopub.status.busy": "2024-08-05T03:07:00.452023Z",
     "iopub.status.idle": "2024-08-05T03:07:07.818958Z",
     "shell.execute_reply": "2024-08-05T03:07:07.818373Z",
     "shell.execute_reply.started": "2024-08-05T03:07:00.452170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA-enabled Network:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type (var_name))                       Output Shape              Param #\n",
       "===============================================================================================\n",
       "Sequential (Sequential)                       [16, 1536, 1536]          --\n",
       "├─Sequential (0)                              [16, 768, 49152]          --\n",
       "│    └─Sequential (0)                         [16, 768, 98304]          --\n",
       "│    │    └─BatchNorm1d (0)                   [16, 768, 98304]          (1,536)\n",
       "│    │    └─GELU (1)                          [16, 768, 98304]          --\n",
       "│    │    └─LoRAConv (2)                      [16, 768, 98304]          92,160\n",
       "│    │    │    └─Conv1d (conv)                [16, 768, 98304]          (2,949,888)\n",
       "│    └─Residual (1)                           [16, 768, 98304]          --\n",
       "│    │    └─Sequential (fn)                   [16, 768, 98304]          --\n",
       "│    │    │    └─BatchNorm1d (0)              [16, 768, 98304]          (1,536)\n",
       "│    │    │    └─GELU (1)                     [16, 768, 98304]          --\n",
       "│    │    │    └─LoRAConv (2)                 [16, 768, 98304]          6,144\n",
       "│    │    │    │    └─Conv1d (conv)           [16, 768, 98304]          (590,592)\n",
       "│    └─AttentionPool (2)                      [16, 768, 49152]          --\n",
       "│    │    └─Rearrange (pool_fn)               [16, 768, 49152, 2]       --\n",
       "│    │    └─LoRAConv (to_attn_logits)         [16, 768, 49152, 2]       6,144\n",
       "│    │    │    └─Conv2d (conv)                [16, 768, 49152, 2]       (589,824)\n",
       "├─Sequential (1)                              [16, 896, 24576]          --\n",
       "│    └─Sequential (0)                         [16, 896, 49152]          --\n",
       "│    │    └─BatchNorm1d (0)                   [16, 768, 49152]          (1,536)\n",
       "│    │    └─GELU (1)                          [16, 768, 49152]          --\n",
       "│    │    └─LoRAConv (2)                      [16, 896, 49152]          94,720\n",
       "│    │    │    └─Conv1d (conv)                [16, 896, 49152]          (3,441,536)\n",
       "│    └─Residual (1)                           [16, 896, 49152]          --\n",
       "│    │    └─Sequential (fn)                   [16, 896, 49152]          --\n",
       "│    │    │    └─BatchNorm1d (0)              [16, 896, 49152]          (1,792)\n",
       "│    │    │    └─GELU (1)                     [16, 896, 49152]          --\n",
       "│    │    │    └─LoRAConv (2)                 [16, 896, 49152]          7,168\n",
       "│    │    │    │    └─Conv1d (conv)           [16, 896, 49152]          (803,712)\n",
       "│    └─AttentionPool (2)                      [16, 896, 24576]          --\n",
       "│    │    └─Rearrange (pool_fn)               [16, 896, 24576, 2]       --\n",
       "│    │    └─LoRAConv (to_attn_logits)         [16, 896, 24576, 2]       7,168\n",
       "│    │    │    └─Conv2d (conv)                [16, 896, 24576, 2]       (802,816)\n",
       "├─Sequential (2)                              [16, 1024, 12288]         --\n",
       "│    └─Sequential (0)                         [16, 1024, 24576]         --\n",
       "│    │    └─BatchNorm1d (0)                   [16, 896, 24576]          (1,792)\n",
       "│    │    └─GELU (1)                          [16, 896, 24576]          --\n",
       "│    │    └─LoRAConv (2)                      [16, 1024, 24576]         110,080\n",
       "│    │    │    └─Conv1d (conv)                [16, 1024, 24576]         (4,588,544)\n",
       "│    └─Residual (1)                           [16, 1024, 24576]         --\n",
       "│    │    └─Sequential (fn)                   [16, 1024, 24576]         --\n",
       "│    │    │    └─BatchNorm1d (0)              [16, 1024, 24576]         (2,048)\n",
       "│    │    │    └─GELU (1)                     [16, 1024, 24576]         --\n",
       "│    │    │    └─LoRAConv (2)                 [16, 1024, 24576]         8,192\n",
       "│    │    │    │    └─Conv1d (conv)           [16, 1024, 24576]         (1,049,600)\n",
       "│    └─AttentionPool (2)                      [16, 1024, 12288]         --\n",
       "│    │    └─Rearrange (pool_fn)               [16, 1024, 12288, 2]      --\n",
       "│    │    └─LoRAConv (to_attn_logits)         [16, 1024, 12288, 2]      8,192\n",
       "│    │    │    └─Conv2d (conv)                [16, 1024, 12288, 2]      (1,048,576)\n",
       "├─Sequential (3)                              [16, 1152, 6144]          --\n",
       "│    └─Sequential (0)                         [16, 1152, 12288]         --\n",
       "│    │    └─BatchNorm1d (0)                   [16, 1024, 12288]         (2,048)\n",
       "│    │    └─GELU (1)                          [16, 1024, 12288]         --\n",
       "│    │    └─LoRAConv (2)                      [16, 1152, 12288]         125,440\n",
       "│    │    │    └─Conv1d (conv)                [16, 1152, 12288]         (5,899,392)\n",
       "│    └─Residual (1)                           [16, 1152, 12288]         --\n",
       "│    │    └─Sequential (fn)                   [16, 1152, 12288]         --\n",
       "│    │    │    └─BatchNorm1d (0)              [16, 1152, 12288]         (2,304)\n",
       "│    │    │    └─GELU (1)                     [16, 1152, 12288]         --\n",
       "│    │    │    └─LoRAConv (2)                 [16, 1152, 12288]         9,216\n",
       "│    │    │    │    └─Conv1d (conv)           [16, 1152, 12288]         (1,328,256)\n",
       "│    └─AttentionPool (2)                      [16, 1152, 6144]          --\n",
       "│    │    └─Rearrange (pool_fn)               [16, 1152, 6144, 2]       --\n",
       "│    │    └─LoRAConv (to_attn_logits)         [16, 1152, 6144, 2]       9,216\n",
       "│    │    │    └─Conv2d (conv)                [16, 1152, 6144, 2]       (1,327,104)\n",
       "├─Sequential (4)                              [16, 1280, 3072]          --\n",
       "│    └─Sequential (0)                         [16, 1280, 6144]          --\n",
       "│    │    └─BatchNorm1d (0)                   [16, 1152, 6144]          (2,304)\n",
       "│    │    └─GELU (1)                          [16, 1152, 6144]          --\n",
       "│    │    └─LoRAConv (2)                      [16, 1280, 6144]          140,800\n",
       "│    │    │    └─Conv1d (conv)                [16, 1280, 6144]          (7,374,080)\n",
       "│    └─Residual (1)                           [16, 1280, 6144]          --\n",
       "│    │    └─Sequential (fn)                   [16, 1280, 6144]          --\n",
       "│    │    │    └─BatchNorm1d (0)              [16, 1280, 6144]          (2,560)\n",
       "│    │    │    └─GELU (1)                     [16, 1280, 6144]          --\n",
       "│    │    │    └─LoRAConv (2)                 [16, 1280, 6144]          10,240\n",
       "│    │    │    │    └─Conv1d (conv)           [16, 1280, 6144]          (1,639,680)\n",
       "│    └─AttentionPool (2)                      [16, 1280, 3072]          --\n",
       "│    │    └─Rearrange (pool_fn)               [16, 1280, 3072, 2]       --\n",
       "│    │    └─LoRAConv (to_attn_logits)         [16, 1280, 3072, 2]       10,240\n",
       "│    │    │    └─Conv2d (conv)                [16, 1280, 3072, 2]       (1,638,400)\n",
       "├─Sequential (5)                              [16, 1536, 1536]          --\n",
       "│    └─Sequential (0)                         [16, 1536, 3072]          --\n",
       "│    │    └─BatchNorm1d (0)                   [16, 1280, 3072]          (2,560)\n",
       "│    │    └─GELU (1)                          [16, 1280, 3072]          --\n",
       "│    │    └─LoRAConv (2)                      [16, 1536, 3072]          158,720\n",
       "│    │    │    └─Conv1d (conv)                [16, 1536, 3072]          (9,831,936)\n",
       "│    └─Residual (1)                           [16, 1536, 3072]          --\n",
       "│    │    └─Sequential (fn)                   [16, 1536, 3072]          --\n",
       "│    │    │    └─BatchNorm1d (0)              [16, 1536, 3072]          (3,072)\n",
       "│    │    │    └─GELU (1)                     [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRAConv (2)                 [16, 1536, 3072]          12,288\n",
       "│    │    │    │    └─Conv1d (conv)           [16, 1536, 3072]          (2,360,832)\n",
       "│    └─AttentionPool (2)                      [16, 1536, 1536]          --\n",
       "│    │    └─Rearrange (pool_fn)               [16, 1536, 1536, 2]       --\n",
       "│    │    └─LoRAConv (to_attn_logits)         [16, 1536, 1536, 2]       12,288\n",
       "│    │    │    └─Conv2d (conv)                [16, 1536, 1536, 2]       (2,359,296)\n",
       "===============================================================================================\n",
       "Total params: 50,477,568\n",
       "Trainable params: 828,416\n",
       "Non-trainable params: 49,649,152\n",
       "Total mult-adds (T): 16.54\n",
       "===============================================================================================\n",
       "Input size (MB): 4831.84\n",
       "Forward/backward pass size (MB): 108112.38\n",
       "Params size (MB): 198.60\n",
       "Estimated Total Size (MB): 113142.81\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoRA-enabled Network:\")\n",
    "summary_on_cuda(\n",
    "    lora_enformer.conv_tower,\n",
    "    input_size=(16, 768, 98_304),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a7a82-c4a7-4423-ba1a-1081b91a517a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T02:11:10.255842Z",
     "iopub.status.busy": "2024-08-05T02:11:10.255664Z",
     "iopub.status.idle": "2024-08-05T02:11:10.258198Z",
     "shell.execute_reply": "2024-08-05T02:11:10.257812Z",
     "shell.execute_reply.started": "2024-08-05T02:11:10.255827Z"
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LoRA Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9c91f-5e5a-458c-b453-c3c7f1c61890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T02:31:05.864592Z",
     "iopub.status.busy": "2024-08-05T02:31:05.864011Z",
     "iopub.status.idle": "2024-08-05T02:31:05.866791Z",
     "shell.execute_reply": "2024-08-05T02:31:05.866375Z",
     "shell.execute_reply.started": "2024-08-05T02:31:05.864575Z"
    }
   },
   "source": [
    "#### Conv Tower Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "547602bc-b57e-4808-abea-244bb675c94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:07.819884Z",
     "iopub.status.busy": "2024-08-05T03:07:07.819732Z",
     "iopub.status.idle": "2024-08-05T03:07:07.823046Z",
     "shell.execute_reply": "2024-08-05T03:07:07.822615Z",
     "shell.execute_reply.started": "2024-08-05T03:07:07.819869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 768\n",
      "out_channels 768\n",
      "groups 1\n",
      "kernel_size (5,)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([20, 3840]) True\n",
      "lora_B torch.Size([768, 20]) True\n",
      "conv.weight torch.Size([768, 768, 5]) False\n",
      "conv.bias torch.Size([768]) False\n",
      "LoRA trainable parameters:  92160\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.conv_tower[0][0][2]\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe907b-6e7b-400b-806d-23009e1f1489",
   "metadata": {},
   "source": [
    "#### Conv Tower Linear Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182c8427-c1f1-4ef5-8e49-da7a59a42f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:07.824202Z",
     "iopub.status.busy": "2024-08-05T03:07:07.824054Z",
     "iopub.status.idle": "2024-08-05T03:07:07.836816Z",
     "shell.execute_reply": "2024-08-05T03:07:07.836416Z",
     "shell.execute_reply.started": "2024-08-05T03:07:07.824189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 768\n",
      "out_channels 768\n",
      "groups 1\n",
      "kernel_size (1,)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([4, 768]) True\n",
      "lora_B torch.Size([768, 4]) True\n",
      "conv.weight torch.Size([768, 768, 1]) False\n",
      "conv.bias torch.Size([768]) False\n",
      "LoRA trainable parameters:  6144\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.conv_tower[0][1].fn[2]\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d0387-75c3-49ec-aee7-5a87c01e7cf8",
   "metadata": {},
   "source": [
    "#### Conv Tower AttentionPool Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a4bae2-5845-42d0-89b2-80d491ca3080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:07.837582Z",
     "iopub.status.busy": "2024-08-05T03:07:07.837448Z",
     "iopub.status.idle": "2024-08-05T03:07:07.843592Z",
     "shell.execute_reply": "2024-08-05T03:07:07.843221Z",
     "shell.execute_reply.started": "2024-08-05T03:07:07.837570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 768\n",
      "out_channels 768\n",
      "groups 1\n",
      "kernel_size (1, 1)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([4, 768]) True\n",
      "lora_B torch.Size([768, 4]) True\n",
      "conv.weight torch.Size([768, 768, 1, 1]) False\n",
      "LoRA trainable parameters:  6144\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.conv_tower[0][2].to_attn_logits\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc29e26-b762-491d-acc0-7e55ad6fc7bf",
   "metadata": {},
   "source": [
    "## Enformer Transformers\n",
    "\n",
    "\n",
    "- Input:\n",
    "  - shape: `(bs, C, seq_len/2**7) or (16, 1536, 1536)`\n",
    "- Output:\n",
    "  - shape: `(bs, C, seq_len/2**7) or (16, 1536, 1536)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94f78f3-526b-4bda-b0ab-26b078486e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:07.844233Z",
     "iopub.status.busy": "2024-08-05T03:07:07.844107Z",
     "iopub.status.idle": "2024-08-05T03:07:09.379988Z",
     "shell.execute_reply": "2024-08-05T03:07:09.379428Z",
     "shell.execute_reply.started": "2024-08-05T03:07:07.844220Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enformer model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                            Output Shape              Param #\n",
       "====================================================================================================\n",
       "Sequential (Sequential)                            [16, 1536, 1536]          --\n",
       "├─Sequential (0)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (1)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (2)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (3)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (4)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (5)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (6)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (7)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (8)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (9)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (10)                                  [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          (6,392,320)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Linear (1)                        [16, 1536, 3072]          (4,721,664)\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─Linear (4)                        [16, 1536, 1536]          (4,720,128)\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "====================================================================================================\n",
       "Total params: 174,242,816\n",
       "Trainable params: 0\n",
       "Non-trainable params: 174,242,816\n",
       "Total mult-adds (G): 6.09\n",
       "====================================================================================================\n",
       "Input size (MB): 151.00\n",
       "Forward/backward pass size (MB): 25606.18\n",
       "Params size (MB): 696.93\n",
       "Estimated Total Size (MB): 26454.10\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEnformer model\")\n",
    "summary_on_cuda(\n",
    "    enformer.transformer,\n",
    "    input_size=(16, 1536, 1536),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533ca0f8-3572-485a-ae73-b69714c75651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:09.380937Z",
     "iopub.status.busy": "2024-08-05T03:07:09.380786Z",
     "iopub.status.idle": "2024-08-05T03:07:10.577965Z",
     "shell.execute_reply": "2024-08-05T03:07:10.577514Z",
     "shell.execute_reply.started": "2024-08-05T03:07:09.380921Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA-enabled Network:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                            Output Shape              Param #\n",
       "====================================================================================================\n",
       "Sequential (Sequential)                            [16, 1536, 1536]          --\n",
       "├─Sequential (0)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (1)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (2)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (3)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (4)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (5)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (6)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (7)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (8)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (9)                                   [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "├─Sequential (10)                                  [16, 1536, 1536]          --\n",
       "│    └─Residual (0)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─Attention (1)                     [16, 1536, 1536]          1,024\n",
       "│    │    │    │    └─LoRALinear (to_q)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_k)            [16, 1536, 512]           794,624\n",
       "│    │    │    │    └─LoRALinear (to_v)            [16, 1536, 1536]          2,371,584\n",
       "│    │    │    │    └─Dropout (pos_dropout)        [3071, 192]               --\n",
       "│    │    │    │    └─LoRALinear (to_rel_k)        [3071, 512]               101,120\n",
       "│    │    │    │    └─Dropout (attn_dropout)       [16, 8, 1536, 1536]       --\n",
       "│    │    │    │    └─LoRALinear (to_out)          [16, 1536, 1536]          2,373,120\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 1536]          --\n",
       "│    └─Residual (1)                                [16, 1536, 1536]          --\n",
       "│    │    └─Sequential (fn)                        [16, 1536, 1536]          --\n",
       "│    │    │    └─LayerNorm (0)                     [16, 1536, 1536]          (3,072)\n",
       "│    │    │    └─LoRALinear (1)                    [16, 1536, 3072]          4,740,096\n",
       "│    │    │    └─Dropout (2)                       [16, 1536, 3072]          --\n",
       "│    │    │    └─ReLU (3)                          [16, 1536, 3072]          --\n",
       "│    │    │    └─LoRALinear (4)                    [16, 1536, 1536]          4,738,560\n",
       "│    │    │    └─Dropout (5)                       [16, 1536, 1536]          --\n",
       "====================================================================================================\n",
       "Total params: 175,129,856\n",
       "Trainable params: 887,040\n",
       "Non-trainable params: 174,242,816\n",
       "Total mult-adds (G): 6.09\n",
       "====================================================================================================\n",
       "Input size (MB): 151.00\n",
       "Forward/backward pass size (MB): 25606.18\n",
       "Params size (MB): 700.47\n",
       "Estimated Total Size (MB): 26457.65\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoRA-enabled Network:\")\n",
    "summary_on_cuda(\n",
    "    lora_enformer.transformer,\n",
    "    input_size=(16, 1536, 1536),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9f434-9476-4ce8-b8dc-c3c23e3eefc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LoRA Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a24fd3-b4e7-4a26-82b8-d2c1d0276057",
   "metadata": {},
   "source": [
    "#### Transformer Q Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13b1e1c-eef1-4e16-96de-f3183db34d41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.578715Z",
     "iopub.status.busy": "2024-08-05T03:07:10.578578Z",
     "iopub.status.idle": "2024-08-05T03:07:10.581478Z",
     "shell.execute_reply": "2024-08-05T03:07:10.581048Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.578701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 1536\n",
      "out_features 512\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([512, 1536]) False\n",
      "lora_A torch.Size([4, 1536]) True\n",
      "lora_B torch.Size([512, 4]) True\n",
      "LoRA trainable parameters:  8192\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][0].fn[1].to_q\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c2071-49a9-4ffe-8b93-3a80fe266d91",
   "metadata": {},
   "source": [
    "#### Transformer K Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b80b717-16f7-47ec-8d7f-3abee5407438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.582094Z",
     "iopub.status.busy": "2024-08-05T03:07:10.581965Z",
     "iopub.status.idle": "2024-08-05T03:07:10.588403Z",
     "shell.execute_reply": "2024-08-05T03:07:10.588004Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.582081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 1536\n",
      "out_features 512\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([512, 1536]) False\n",
      "lora_A torch.Size([4, 1536]) True\n",
      "lora_B torch.Size([512, 4]) True\n",
      "LoRA trainable parameters:  8192\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][0].fn[1].to_k\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fa53e-9eda-4afb-8a5f-0e7c869cbdc7",
   "metadata": {},
   "source": [
    "#### Transformer V Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7cf8e62-2892-4814-9fb4-5ca5a42cc62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.590016Z",
     "iopub.status.busy": "2024-08-05T03:07:10.589869Z",
     "iopub.status.idle": "2024-08-05T03:07:10.594580Z",
     "shell.execute_reply": "2024-08-05T03:07:10.594188Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.590003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 1536\n",
      "out_features 1536\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([1536, 1536]) False\n",
      "lora_A torch.Size([4, 1536]) True\n",
      "lora_B torch.Size([1536, 4]) True\n",
      "LoRA trainable parameters:  12288\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][0].fn[1].to_v\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4989f23-e699-4213-8bf9-df57f45f9eeb",
   "metadata": {},
   "source": [
    "#### Transformer Rel K Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b880ade-9825-41a4-8973-6d072fa1a370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.595223Z",
     "iopub.status.busy": "2024-08-05T03:07:10.595094Z",
     "iopub.status.idle": "2024-08-05T03:07:10.601210Z",
     "shell.execute_reply": "2024-08-05T03:07:10.600820Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.595210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 192\n",
      "out_features 512\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([512, 192]) False\n",
      "lora_A torch.Size([4, 192]) True\n",
      "lora_B torch.Size([512, 4]) True\n",
      "LoRA trainable parameters:  2816\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][0].fn[1].to_rel_k\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc0619-b212-450a-9c6b-c69d915086e9",
   "metadata": {},
   "source": [
    "#### Transformer Output Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247cb6c7-b881-43f2-889e-5a374d14a966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.601845Z",
     "iopub.status.busy": "2024-08-05T03:07:10.601718Z",
     "iopub.status.idle": "2024-08-05T03:07:10.621016Z",
     "shell.execute_reply": "2024-08-05T03:07:10.620621Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.601832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 1536\n",
      "out_features 1536\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([1536, 1536]) False\n",
      "bias torch.Size([1536]) False\n",
      "lora_A torch.Size([4, 1536]) True\n",
      "lora_B torch.Size([1536, 4]) True\n",
      "LoRA trainable parameters:  12288\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][0].fn[1].to_out\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44dc8a0-031b-4d0a-8616-b88c20318805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T02:22:53.864260Z",
     "iopub.status.busy": "2024-08-05T02:22:53.863768Z",
     "iopub.status.idle": "2024-08-05T02:22:53.866302Z",
     "shell.execute_reply": "2024-08-05T02:22:53.865879Z",
     "shell.execute_reply.started": "2024-08-05T02:22:53.864244Z"
    }
   },
   "source": [
    "#### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3251d0ed-d137-4110-bd30-585608611682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.621797Z",
     "iopub.status.busy": "2024-08-05T03:07:10.621665Z",
     "iopub.status.idle": "2024-08-05T03:07:10.627589Z",
     "shell.execute_reply": "2024-08-05T03:07:10.627185Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.621784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 1536\n",
      "out_features 3072\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([3072, 1536]) False\n",
      "bias torch.Size([3072]) False\n",
      "lora_A torch.Size([4, 1536]) True\n",
      "lora_B torch.Size([3072, 4]) True\n",
      "LoRA trainable parameters:  18432\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][1].fn[1]\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8982c112-71f8-4a60-bca7-e0644e5da8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.628222Z",
     "iopub.status.busy": "2024-08-05T03:07:10.628094Z",
     "iopub.status.idle": "2024-08-05T03:07:10.634010Z",
     "shell.execute_reply": "2024-08-05T03:07:10.633625Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.628209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 3072\n",
      "out_features 1536\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([1536, 3072]) False\n",
      "bias torch.Size([1536]) False\n",
      "lora_A torch.Size([4, 3072]) True\n",
      "lora_B torch.Size([1536, 4]) True\n",
      "LoRA trainable parameters:  18432\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.transformer[0][1].fn[4]\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e35ee-7fcc-4f8e-8d85-3dff273bece4",
   "metadata": {},
   "source": [
    "## Enformer Final Point Wise\n",
    "\n",
    "- Input:\n",
    "  - shape: `(bs, C, seq_len/2**7) or (16, 1536, 1536)`\n",
    "- Output:\n",
    "  - shape: `(bs, seq_len/2**7, 2*C) or (16, 1536, 3072)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea3171bb-2cd1-4ed7-a074-7ecdc48f7c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.634648Z",
     "iopub.status.busy": "2024-08-05T03:07:10.634519Z",
     "iopub.status.idle": "2024-08-05T03:07:10.884287Z",
     "shell.execute_reply": "2024-08-05T03:07:10.883839Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.634636Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enformer model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 3072]          --\n",
       "├─Rearrange (0)                          [16, 1536, 1536]          --\n",
       "├─Sequential (1)                         [16, 3072, 1536]          --\n",
       "│    └─BatchNorm1d (0)                   [16, 1536, 1536]          (3,072)\n",
       "│    └─GELU (1)                          [16, 1536, 1536]          --\n",
       "│    └─Conv1d (2)                        [16, 3072, 1536]          (4,721,664)\n",
       "├─Rearrange (2)                          [16, 1536, 3072]          --\n",
       "├─Dropout (3)                            [16, 1536, 3072]          --\n",
       "├─GELU (4)                               [16, 1536, 3072]          --\n",
       "==========================================================================================\n",
       "Total params: 4,724,736\n",
       "Trainable params: 0\n",
       "Non-trainable params: 4,724,736\n",
       "Total mult-adds (G): 116.04\n",
       "==========================================================================================\n",
       "Input size (MB): 151.00\n",
       "Forward/backward pass size (MB): 905.97\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 1075.86\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEnformer model\")\n",
    "summary_on_cuda(\n",
    "    enformer.final_pointwise,\n",
    "    input_size=(16, 1536, 1536),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c4171c7-b324-4c13-938b-ac6df158d716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:10.885004Z",
     "iopub.status.busy": "2024-08-05T03:07:10.884867Z",
     "iopub.status.idle": "2024-08-05T03:07:11.124188Z",
     "shell.execute_reply": "2024-08-05T03:07:11.123754Z",
     "shell.execute_reply.started": "2024-08-05T03:07:10.884991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA-enabled Network:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 3072]          --\n",
       "├─Rearrange (0)                          [16, 1536, 1536]          --\n",
       "├─Sequential (1)                         [16, 3072, 1536]          --\n",
       "│    └─BatchNorm1d (0)                   [16, 1536, 1536]          (3,072)\n",
       "│    └─GELU (1)                          [16, 1536, 1536]          --\n",
       "│    └─LoRAConv (2)                      [16, 3072, 1536]          18,432\n",
       "│    │    └─Conv1d (conv)                [16, 3072, 1536]          (4,721,664)\n",
       "├─Rearrange (2)                          [16, 1536, 3072]          --\n",
       "├─Dropout (3)                            [16, 1536, 3072]          --\n",
       "├─GELU (4)                               [16, 1536, 3072]          --\n",
       "==========================================================================================\n",
       "Total params: 4,743,168\n",
       "Trainable params: 18,432\n",
       "Non-trainable params: 4,724,736\n",
       "Total mult-adds (G): 116.04\n",
       "==========================================================================================\n",
       "Input size (MB): 151.00\n",
       "Forward/backward pass size (MB): 905.97\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 1075.86\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoRA-enabled Network:\")\n",
    "summary_on_cuda(\n",
    "    lora_enformer.final_pointwise,\n",
    "    input_size=(16, 1536, 1536),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787e274-4bc8-41d4-b73a-8a18f01bcebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T02:34:58.530842Z",
     "iopub.status.busy": "2024-08-05T02:34:58.530670Z",
     "iopub.status.idle": "2024-08-05T02:34:58.535962Z",
     "shell.execute_reply": "2024-08-05T02:34:58.535551Z",
     "shell.execute_reply.started": "2024-08-05T02:34:58.530828Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "source": [
    "### LoRA Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83057928-c69a-49fd-ae7b-0e815f7af04a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:11.124914Z",
     "iopub.status.busy": "2024-08-05T03:07:11.124770Z",
     "iopub.status.idle": "2024-08-05T03:07:11.127471Z",
     "shell.execute_reply": "2024-08-05T03:07:11.127065Z",
     "shell.execute_reply.started": "2024-08-05T03:07:11.124900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 1536\n",
      "out_channels 3072\n",
      "groups 1\n",
      "kernel_size (1,)\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRAConv'>\n",
      "lora_A torch.Size([4, 1536]) True\n",
      "lora_B torch.Size([3072, 4]) True\n",
      "conv.weight torch.Size([3072, 1536, 1]) False\n",
      "conv.bias torch.Size([3072]) False\n",
      "LoRA trainable parameters:  18432\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.final_pointwise[1][2]\n",
    "print_lora_conv(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d43a40-855b-493e-af3e-b14e3831ab19",
   "metadata": {},
   "source": [
    "## Enformer Output Heads - Human\n",
    "\n",
    "- Input:\n",
    "  - shape: `(bs, seq_len/2**7, 2*C) or (16, 1536, 3072)`\n",
    "- Output:\n",
    "  - shape: `(bs, seq_len/2**7, n_tracks) or (16, 1536, 5313)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "671503c5-fec4-4d7b-a297-395188bbe4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:11.128112Z",
     "iopub.status.busy": "2024-08-05T03:07:11.127987Z",
     "iopub.status.idle": "2024-08-05T03:07:11.648592Z",
     "shell.execute_reply": "2024-08-05T03:07:11.648152Z",
     "shell.execute_reply.started": "2024-08-05T03:07:11.128100Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enformer model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 5313]          --\n",
       "├─Linear (0)                             [16, 1536, 5313]          (16,326,849)\n",
       "├─Softplus (1)                           [16, 1536, 5313]          --\n",
       "==========================================================================================\n",
       "Total params: 16,326,849\n",
       "Trainable params: 0\n",
       "Non-trainable params: 16,326,849\n",
       "Total mult-adds (M): 261.23\n",
       "==========================================================================================\n",
       "Input size (MB): 301.99\n",
       "Forward/backward pass size (MB): 1044.58\n",
       "Params size (MB): 65.31\n",
       "Estimated Total Size (MB): 1411.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEnformer model\")\n",
    "summary_on_cuda(\n",
    "    enformer.heads[\"human\"],\n",
    "    input_size=(16, 1536, 3072),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cafe8a2f-f758-4cc9-a877-5ac55cb0997d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:11.649344Z",
     "iopub.status.busy": "2024-08-05T03:07:11.649203Z",
     "iopub.status.idle": "2024-08-05T03:07:12.162298Z",
     "shell.execute_reply": "2024-08-05T03:07:12.161818Z",
     "shell.execute_reply.started": "2024-08-05T03:07:11.649330Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA-enabled Network:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 5313]          --\n",
       "├─LoRALinear (0)                         [16, 1536, 5313]          16,360,389\n",
       "├─Softplus (1)                           [16, 1536, 5313]          --\n",
       "==========================================================================================\n",
       "Total params: 16,360,389\n",
       "Trainable params: 33,540\n",
       "Non-trainable params: 16,326,849\n",
       "Total mult-adds (M): 261.23\n",
       "==========================================================================================\n",
       "Input size (MB): 301.99\n",
       "Forward/backward pass size (MB): 1044.58\n",
       "Params size (MB): 65.44\n",
       "Estimated Total Size (MB): 1412.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoRA-enabled Network:\")\n",
    "summary_on_cuda(\n",
    "    lora_enformer.heads[\"human\"],\n",
    "    input_size=(16, 1536, 3072),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f2219-62dd-4ae4-8338-927e0abff8f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LoRA Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9989cb-a770-4019-be4e-f390dd50e807",
   "metadata": {},
   "source": [
    "#### Output Head Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e9d75fc-4f8e-41a9-810a-6b78720de61d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:12.163100Z",
     "iopub.status.busy": "2024-08-05T03:07:12.162949Z",
     "iopub.status.idle": "2024-08-05T03:07:12.165822Z",
     "shell.execute_reply": "2024-08-05T03:07:12.165426Z",
     "shell.execute_reply.started": "2024-08-05T03:07:12.163086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 3072\n",
      "out_features 5313\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([5313, 3072]) False\n",
      "bias torch.Size([5313]) False\n",
      "lora_A torch.Size([4, 3072]) True\n",
      "lora_B torch.Size([5313, 4]) True\n",
      "LoRA trainable parameters:  33540\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.heads[\"human\"][0]\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa8b2f-b86e-4d3d-8dce-f8e2c574976c",
   "metadata": {},
   "source": [
    "## Enformer Output Heads - Mouse:\n",
    "  - Input:\n",
    "    - shape: `(bs, seq_len/2**7, 2*C) or (16, 1536, 3072)`\n",
    "  - Output:\n",
    "    - shape: `(bs, seq_len/2**7, n_tracks) or (16, 1536, 1643)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6790c9b3-33f7-4ed8-8137-f97dbd24cc09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:12.166538Z",
     "iopub.status.busy": "2024-08-05T03:07:12.166402Z",
     "iopub.status.idle": "2024-08-05T03:07:12.648500Z",
     "shell.execute_reply": "2024-08-05T03:07:12.648055Z",
     "shell.execute_reply.started": "2024-08-05T03:07:12.166526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enformer model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 1643]          --\n",
       "├─Linear (0)                             [16, 1536, 1643]          (5,048,939)\n",
       "├─Softplus (1)                           [16, 1536, 1643]          --\n",
       "==========================================================================================\n",
       "Total params: 5,048,939\n",
       "Trainable params: 0\n",
       "Non-trainable params: 5,048,939\n",
       "Total mult-adds (M): 80.78\n",
       "==========================================================================================\n",
       "Input size (MB): 301.99\n",
       "Forward/backward pass size (MB): 323.03\n",
       "Params size (MB): 20.20\n",
       "Estimated Total Size (MB): 645.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEnformer model\")\n",
    "summary_on_cuda(\n",
    "    enformer.heads[\"mouse\"],\n",
    "    input_size=(16, 1536, 3072),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35cdb8d1-897e-4853-9091-6f58e02ab4ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:12.649272Z",
     "iopub.status.busy": "2024-08-05T03:07:12.649122Z",
     "iopub.status.idle": "2024-08-05T03:07:13.098227Z",
     "shell.execute_reply": "2024-08-05T03:07:13.097784Z",
     "shell.execute_reply.started": "2024-08-05T03:07:12.649258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA-enabled Network:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type (var_name))                  Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential (Sequential)                  [16, 1536, 1643]          --\n",
       "├─LoRALinear (0)                         [16, 1536, 1643]          5,067,799\n",
       "├─Softplus (1)                           [16, 1536, 1643]          --\n",
       "==========================================================================================\n",
       "Total params: 5,067,799\n",
       "Trainable params: 18,860\n",
       "Non-trainable params: 5,048,939\n",
       "Total mult-adds (M): 80.78\n",
       "==========================================================================================\n",
       "Input size (MB): 301.99\n",
       "Forward/backward pass size (MB): 323.03\n",
       "Params size (MB): 20.27\n",
       "Estimated Total Size (MB): 645.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLoRA-enabled Network:\")\n",
    "summary_on_cuda(\n",
    "    lora_enformer.heads[\"mouse\"],\n",
    "    input_size=(16, 1536, 3072),\n",
    "    row_settings=[\"var_names\"],\n",
    "    depth=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a8fb4-1dfe-43ab-a791-627c3db63d93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LoRA Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548b546-ac9d-401c-a862-3201ec58608f",
   "metadata": {},
   "source": [
    "#### Output Head Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06e6d712-d08a-4c74-9d1b-a245ea5da5e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T03:07:13.098974Z",
     "iopub.status.busy": "2024-08-05T03:07:13.098827Z",
     "iopub.status.idle": "2024-08-05T03:07:13.101574Z",
     "shell.execute_reply": "2024-08-05T03:07:13.101174Z",
     "shell.execute_reply.started": "2024-08-05T03:07:13.098960Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features 3072\n",
      "out_features 1643\n",
      "\n",
      "LoRA Module <class 'bolero.tl.generic.module_lora.LoRALinear'>\n",
      "weight torch.Size([1643, 3072]) False\n",
      "bias torch.Size([1643]) False\n",
      "lora_A torch.Size([4, 3072]) True\n",
      "lora_B torch.Size([1643, 4]) True\n",
      "LoRA trainable parameters:  18860\n"
     ]
    }
   ],
   "source": [
    "_module = lora_enformer.heads[\"mouse\"][0]\n",
    "print_lora_linear(_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712aae2-acac-4278-a131-adcb70e51758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
